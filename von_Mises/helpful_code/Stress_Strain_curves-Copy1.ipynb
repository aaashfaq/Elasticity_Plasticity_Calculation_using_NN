{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f91e9246-e6fe-4669-a363-c2f35fd81226",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-20 09:29:53.056831: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-11-20 09:29:53.059151: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-11-20 09:29:53.083808: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-11-20 09:29:53.111262: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-11-20 09:29:53.140662: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-11-20 09:29:53.149690: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-20 09:29:53.196844: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-11-20 09:29:53.908624: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyDOE import lhs  # Latin Hypercube Sampling\n",
    "from scipy.optimize import newton\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "\n",
    "# Material constants from Table 4\n",
    "E = 193000       # MPa\n",
    "sigma_y = 150    # MPa\n",
    "c1 = 230000      # MPa\n",
    "c2 = 19600       # MPa\n",
    "c3 = 1800        # MPa\n",
    "gamma1 = 1200\n",
    "gamma2 = 140\n",
    "gamma3 = 4\n",
    "b = 8\n",
    "Q = 110          # MPastress_values_predicted = calculate_stress_from_plastic_strain(y_pred_denormalized, strain_values_test, material_constants)\n",
    "\n",
    "# Number of material samples (NLHS) and strain samples (NΔε)\n",
    "NLHS = 1\n",
    "NΔε = 1500\n",
    "\n",
    "# Generate fixed material constants from Table 4\n",
    "def generate_material_constants(num_samples):\n",
    "    # Create arrays of constants repeated for num_samples\n",
    "    E_array = np.full(num_samples, E)\n",
    "    sigma_y_array = np.full(num_samples, sigma_y)\n",
    "    c1_array = np.full(num_samples, c1)\n",
    "    c2_array = np.full(num_samples, c2)\n",
    "    c3_array = np.full(num_samples, c3)\n",
    "    gamma1_array = np.full(num_samples, gamma1)\n",
    "    gamma2_array = np.full(num_samples, gamma2)\n",
    "    gamma3_array = np.full(num_samples, gamma3)\n",
    "    b_array = np.full(num_samples, b)\n",
    "    Q_array = np.full(num_samples, Q)\n",
    "    \n",
    "    # Stack them together as columns\n",
    "    return np.column_stack((E_array, sigma_y_array, c1_array, c2_array, c3_array,\n",
    "                            gamma1_array, gamma2_array, gamma3_array, b_array, Q_array))\n",
    "\n",
    "\n",
    "# Generate strain history by accumulating increments\n",
    "def generate_strain_history(num_samples, lb, ub):\n",
    "    strain_history = np.zeros(num_samples)\n",
    "    cumulative_strain = 0  # Start with zero cumulative strain\n",
    "\n",
    "    # First range: Accumulate small positive strain increments\n",
    "    for i in range(300):\n",
    "        increment = np.random.uniform(lb, ub)\n",
    "        cumulative_strain += increment\n",
    "        strain_history[i] = cumulative_strain\n",
    "\n",
    "    # Second range: Accumulate negative strain increments\n",
    "    for i in range(300, 900):\n",
    "        increment = np.random.uniform(lb, ub)\n",
    "        cumulative_strain -= increment\n",
    "        strain_history[i] = cumulative_strain\n",
    "\n",
    "    # Third range: Return to positive strain increments\n",
    "    for i in range(900, num_samples):\n",
    "        increment = np.random.uniform(lb, ub)\n",
    "        cumulative_strain += increment\n",
    "        strain_history[i] = cumulative_strain\n",
    "\n",
    "    return strain_history\n",
    "\n",
    "# Hooke's law (elastic predictor)\n",
    "def elastic_predictor(eps, eps_p, E, sigma_k1, sigma_k2, sigma_k3, sigma_i, sigma_y):\n",
    "    sigma_trial = E * (eps - eps_p)  # Trial stress\n",
    "    back_stress = sigma_k1 + sigma_k2 + sigma_k3\n",
    "    yield_function = np.abs(sigma_trial - back_stress) - (sigma_y + sigma_i)\n",
    "    return sigma_trial, yield_function\n",
    "\n",
    "# Update hardening rules\n",
    "def update_kinematic_hardening(sigma_k, eps_p_dot, c, gamma):\n",
    "    return (2.0 / 3.0) * c * eps_p_dot - gamma * sigma_k * np.abs(eps_p_dot)\n",
    "\n",
    "def update_isotropic_hardening(eps_p_dot, sigma_i, b, Q):\n",
    "    return b * (Q - sigma_i) * np.abs(eps_p_dot)\n",
    "\n",
    "# Return mapping algorithm\n",
    "def return_mapping(eps, eps_p, sigma_trial, yield_function, sigma_k1, sigma_k2, sigma_k3, sigma_i, c1, c2, c3, gamma1, gamma2, gamma3, b, Q, E, sigma_y):\n",
    "    if yield_function <= 0:\n",
    "        # Elastic step\n",
    "        return sigma_trial, eps_p, sigma_k1, sigma_k2, sigma_k3, sigma_i\n",
    "    else:\n",
    "        # Plastic corrector step\n",
    "        def plastic_residual(delta_gamma):\n",
    "            eps_p_dot = delta_gamma\n",
    "            back_stress = sigma_k1 + sigma_k2 + sigma_k3\n",
    "            sigma_updated = sigma_trial - E * delta_gamma\n",
    "            yield_function_updated = np.abs(sigma_updated - back_stress) - (sigma_y + sigma_i + b * delta_gamma)\n",
    "            return yield_function_updated\n",
    "\n",
    "        try:\n",
    "            delta_gamma = newton(plastic_residual, 1e-5, tol=1e-8, maxiter=50)  # Solve for plastic multiplier\n",
    "        except RuntimeError:\n",
    "            delta_gamma = 0  # If Newton-Raphson fails, assume no plastic deformation\n",
    "\n",
    "        eps_p += delta_gamma\n",
    "        sigma_k1 += update_kinematic_hardening(sigma_k1, delta_gamma, c1, gamma1)\n",
    "        sigma_k2 += update_kinematic_hardening(sigma_k2, delta_gamma, c2, gamma2)\n",
    "        sigma_k3 += update_kinematic_hardening(sigma_k3, delta_gamma, c3, gamma3)\n",
    "        sigma_i += update_isotropic_hardening(delta_gamma, sigma_i, b, Q)\n",
    "\n",
    "        # Final stress update\n",
    "        sigma_updated = sigma_trial - E * delta_gamma\n",
    "        return sigma_updated, eps_p, sigma_k1, sigma_k2, sigma_k3, sigma_i\n",
    "\n",
    "def generate_dataset(lb, ub, num_material_samples=1, num_strain_samples=1500):\n",
    "    material_constants = generate_material_constants(num_material_samples)\n",
    "    # Generate different strain histories for each sample if desired\n",
    "    # Or use the same strain history for all samples\n",
    "    \n",
    "    dataset = []\n",
    "    \n",
    "    for i in range(num_material_samples):\n",
    "        E, sigma_y = material_constants[i, 0], material_constants[i, 1]\n",
    "        c = material_constants[i, 2:5]\n",
    "        gamma = material_constants[i, 5:8]\n",
    "        b, Q = material_constants[i, 8], material_constants[i, 9]\n",
    "        \n",
    "        # Initialize state variables\n",
    "        sigma_k1, sigma_k2, sigma_k3, sigma_i, eps_p = 0, 0, 0, 0, 0\n",
    "        \n",
    "        # Generate strain history for each material sample if desired\n",
    "        strain_history = generate_strain_history(num_strain_samples, lb, ub)\n",
    "        \n",
    "        for j in range(num_strain_samples):\n",
    "            eps = strain_history[j]\n",
    "            # Elastic predictor\n",
    "            sigma_trial, yield_function = elastic_predictor(eps, eps_p, E, sigma_k1, sigma_k2, sigma_k3, sigma_i, sigma_y)\n",
    "\n",
    "            # Run the return mapping algorithm\n",
    "            sigma_updated, eps_p, sigma_k1, sigma_k2, sigma_k3, sigma_i = return_mapping(\n",
    "                eps, eps_p, sigma_trial, yield_function, sigma_k1, sigma_k2, sigma_k3, sigma_i, \n",
    "                c[0], c[1], c[2], gamma[0], gamma[1], gamma[2], b, Q, E, sigma_y\n",
    "            )\n",
    "            \n",
    "\n",
    "            # Calculate trial phi (trialϕ)\n",
    "            #trial_phi = np.abs(sigma_trial) - sigma_y\n",
    "            back_stress = sigma_k1 + sigma_k2 + sigma_k3\n",
    "            trial_phi = np.abs(sigma_trial - back_stress) - (sigma_y + sigma_i)\n",
    "\n",
    "            \n",
    "            # Compute stress using the return mapping algorithm\n",
    "            stress_rm = sigma_updated  # Stress from Return Mapping\n",
    "\n",
    "            # Store the strain and stress\n",
    "            dataset.append([\n",
    "                E, sigma_y, c[0],gamma[0], c[1],gamma[1], c[2],gamma[2],\n",
    "                b, Q, trial_phi,\n",
    "                eps,          # Strain\n",
    "                stress_rm,    # Stress from Return Mapping\n",
    "                np.abs(eps_p)\n",
    "                #round(eps_p, 5)         # Plastic strain\n",
    "            ])\n",
    "    dataset = np.array(dataset)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "# Save the dataset to a CSV file\n",
    "def save_dataset_to_csv(dataset, filename=\"generated_dataset_training_check_07_11.csv\"):\n",
    "    columns = ['E', 'sigma_y', 'c1', 'c2', 'c3',\n",
    "               'gamma1', 'gamma2', 'gamma3',\n",
    "               'b', 'Q','trial_phi', 'strain', 'stress_rm', 'plastic_strain']\n",
    "    df = pd.DataFrame(dataset, columns=columns)\n",
    "    df.to_csv(filename, index=False)\n",
    "    print(f\"Dataset saved to {filename}\")\n",
    "\n",
    "# Normalize the dataset\n",
    "def normalize_dataset(data, scaler=None):\n",
    "    if scaler is None:\n",
    "        scaler = MinMaxScaler()\n",
    "        normalized_data = scaler.fit_transform(data)\n",
    "    else:\n",
    "        normalized_data = scaler.transform(data)\n",
    "    return normalized_data, scaler\n",
    "\n",
    "\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "def split_dataset(X, y):\n",
    "    return train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Build the neural network\n",
    "def build_model(input_dim):\n",
    "    model = Sequential([\n",
    "        #Dense(100, activation='sigmoid', input_shape=(input_dim,)),  # Use 10 neurons, sigmoid activation\n",
    "        #Dense(500, activation='sigmoid', input_shape=(input_dim,)),  # Use 10 neurons, sigmoid activation\n",
    "        Dense(10, activation='sigmoid'),  # Second hidden layer also with 10 neurons\n",
    "        Dense(1, activation='linear')     # Linear activation for the output\n",
    "    ])\n",
    "    custom_adam = tf.keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-8)\n",
    "    model.compile(optimizer=custom_adam, loss='mean_squared_error', metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "# Plot training and validation loss\n",
    "def plot_loss(history, filename='loss_curve.png'):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(history.history['loss'], label='Training Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss (MSE)')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    #plt.savefig(filename)  # Save the plot as an image file\n",
    "    #plt.close()  # Close the plot to avoid displaying it in the notebook\n",
    "    #print(f\"Loss curve saved as {filename}\")\n",
    "\n",
    "# Plot predicted vs original values\n",
    "def plot_predicted_vs_actual(y_test, y_pred, filename='predicted_vs_actual.png'):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(y_test, label='Original Values', color='blue')\n",
    "    plt.plot(y_pred, label='Predicted Values', color='red', linestyle='--')\n",
    "    plt.xlabel('Sample index')\n",
    "    plt.ylabel('Plastic Strain')\n",
    "    plt.title('Predicted vs. Original Plastic Strain')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    plt.savefig('predicted_vs_actual')  # Save the plot as an image file\n",
    "    plt.close()  # Close the plot to avoid displaying it in the notebookLHSLHS\n",
    "    print(f\"Plot saved as {'predicted_vs_actual'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6ab32db-214b-4f2f-b0ba-e8010cd42d28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating training dataset...\n"
     ]
    }
   ],
   "source": [
    "# Generate training dataset\n",
    "print(\"Generating training dataset...\")\n",
    "training = generate_dataset(lb=0.0001, ub=0.0002, num_material_samples=NLHS, num_strain_samples=NΔε)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e13a542-9e76-4038-b806-c25d6b80cfcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "training= normalize_dataset(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb1d687d-b6b1-4593-bb61-7c53948c74d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#training, sclaer = normalize_dataset(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aedd62d9-c0fc-409e-858f-5b46647df095",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'iloc'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Separate features and target\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m X \u001b[38;5;241m=\u001b[39m training\u001b[38;5;241m.\u001b[39miloc[:, : \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m3\u001b[39m]  \u001b[38;5;66;03m# Features (E, sigma_y, c1, c2, c3, gamma1, gamma2, gamma3, b, Q, strain)\u001b[39;00m\n\u001b[1;32m      3\u001b[0m y \u001b[38;5;241m=\u001b[39m training\u001b[38;5;241m.\u001b[39miloc[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'iloc'"
     ]
    }
   ],
   "source": [
    "# Separate features and target\n",
    "X = training[:, : -3]  # Features (E, sigma_y, c1, c2, c3, gamma1, gamma2, gamma3, b, Q, strain)\n",
    "y = training[:, -1]   # Targets (plastic_strain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2233572f-3c8a-43a7-bc95-48a8a1f7dc70",
   "metadata": {},
   "outputs": [],
   "source": [
    "X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14bc8fa-77f9-40ad-a4d5-052c0b0c3619",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Normalize the dataset\n",
    "print(\"Normalizing the dataset...\")\n",
    "#X_normalized, scaler_X = normalize_dataset(X)\n",
    "X_normalized=X\n",
    "y = y.reshape(-1, 1)\n",
    "#y_normalized, scaler_y = normalize_dataset(y)\n",
    "y_normalized = y\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "print(\"Splitting the dataset into training and testing sets...\")\n",
    "X_train, X_test, y_train, y_test = split_dataset(X_normalized, y_normalized)\n",
    "\n",
    "# Build the neural network model\n",
    "print(\"Building the neural network model...\")\n",
    "model = build_model(X_train.shape[1])\n",
    "\n",
    "# Define the early stopping callback\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',         # Monitor the validation loss\n",
    "    patience=50,                # Number of epochs with no improvement after which training will be stopped\n",
    "    restore_best_weights=True   # Restore model weights from the epoch with the best value of the monitored quantity\n",
    ")\n",
    "\n",
    "# Train the model with early stopping\n",
    "print(\"Training the model...\")\n",
    "history = model.fit(\n",
    "    X_train, \n",
    "    y_train, \n",
    "    epochs=1000,               # Set a large number of epochs\n",
    "    batch_size=1000,\n",
    "    validation_split=0.2, \n",
    "    callbacks=[early_stopping],  # Include the early stopping callback\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593d69fc-375b-4255-bd11-33f65752c984",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test set\n",
    "print(\"Evaluating the model on the test set...\")\n",
    "test_loss, test_mae = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"Test MAE: {test_mae}\")\n",
    "\n",
    "# Make predictionsarr_reduced = arr[0, :]\n",
    "print(\"Making predictions on the test set...\")\n",
    "y_pred_normalized = np.abs(model.predict(X_test))\n",
    "\n",
    "# Denormalize predictions and actual values\n",
    "#y_pred_denormalized = scaler_y.inverse_transform(y_pred_normalized).flatten()\n",
    "#y_test_denormalized = scaler_y.inverse_transform(y_test).flatten()\n",
    "\n",
    "# Save the model\n",
    "model.save('plastic_strain_predictor_model.h5')\n",
    "print(\"Model saved as 'plastic_strain_predictor_model.h5'\")\n",
    "\n",
    "# Plot training and validation loss\n",
    "plot_loss(history, filename='loss_curve.png')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b89a485d-bfae-4636-b42e-d479a221e8d1",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "y_pred_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90ce9cc-4c51-4014-ae3a-eaf6f8aac48d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d11663-6a8e-4491-9dfd-25fd08af8b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot predicted vs original plastic strain\n",
    "plot_predicted_vs_actual(y_test, y_pred_normalized, filename='predicted_vs_actual.png')\n",
    "#plot_predicted_vs_actual(y_test, y_pred_de, filename='predicted_vs_actual.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "981216b7-16c7-4e0c-911e-e346e36359d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate stress from predicted plastic strain values using return mapping\n",
    "def calculate_stress_from_plastic_strain(plastic_strain_values, strain_values, material_constants):\n",
    "    # Unpack material constants\n",
    "    E, sigma_y, c1, c2, c3, gamma1, gamma2, gamma3, b, Q = material_constants\n",
    "    \n",
    "    # Initialize variables for kinematic and isotropic hardening\n",
    "    sigma_k1, sigma_k2, sigma_k3, sigma_i, eps_p = 0, 0, 0, 0, 0\n",
    "    \n",
    "    stress_values = []\n",
    "    \n",
    "    # Loop through each strain and corresponding predicted plastic strain\n",
    "    for i in range(len(plastic_strain_values)):\n",
    "        eps_p_nn = plastic_strain_values[i]  # Predicted plastic strain from the neural network\n",
    "        eps = strain_values[i]               # Total strain (input strain)\n",
    "        \n",
    "        # Elastic predictor step\n",
    "        sigma_trial, yield_function = elastic_predictor(eps, eps_p_nn, E, sigma_k1, sigma_k2, sigma_k3, sigma_i, sigma_y)\n",
    "\n",
    "        if yield_function <= 0:\n",
    "            sigma_updated=sigma_trial\n",
    "        else:      \n",
    "            # Return mapping algorithm to calculate final stress\n",
    "            sigma_updated, eps_p, sigma_k1, sigma_k2, sigma_k3, sigma_i = return_mapping(\n",
    "            eps, eps_p_nn, sigma_trial, yield_function, sigma_k1, sigma_k2, sigma_k3, sigma_i, \n",
    "            c1, c2, c3, gamma1, gamma2, gamma3, b, Q, E, sigma_y)\n",
    "\n",
    "        # Append the updated stress value to the list\n",
    "        stress_values.append(sigma_updated)\n",
    "    \n",
    "    return np.array(stress_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace845f4-aac0-40ac-b6fc-f2d1ce171729",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get strain values from the test set (or use the\n",
    "# Define material constants (same as those used for training)\n",
    "material_constants = [E, sigma_y, c1, c2, c3, gamma1, gamma2, gamma3, b, Q]\n",
    "\n",
    "# Get strain values from the test set (or use the strain values corresponding to the predicted plastic strain)\n",
    "strain_values_test = training[:, 11]  # Assuming the last column of X_test is the strain values\n",
    "\n",
    "y_pred_normalized_nn = model.predict(X_normalized)\n",
    "#y_pred_normalized_nn = training[:,-1]\n",
    "y_pred_normalized_nn = np.squeeze(y_pred_normalized_nn)\n",
    "\n",
    "\n",
    "#stress_values_predicted = calculate_stress_from_plastic_strain(y_pred_normalized_nn, strain_values_test, material_constants)\n",
    "stress_values_predicted = 193000*(strain_values_test - y_pred_normalized_nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372de5c5-1ac1-4fd1-8147-e86a118b0768",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot predicted stress vs strain\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(strain_values_test, stress_values_predicted, label='Predicted Stress', color='red')\n",
    "#plt.plot(strain_values_test, training[:,-2], label='Actual Stress', color='blue')\n",
    "plt.xlabel('Strain')\n",
    "plt.ylabel('Stress (MPa)')\n",
    "plt.title('Predicted Stress-Strain Curve')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "41511723-14f9-4ef0-ae79-d9b305730968",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'strain_values_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m8\u001b[39m, \u001b[38;5;241m6\u001b[39m))\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#plt.plot(strain_values_test, stress_values_predicted, label='Predicted Stress', color='red')\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(strain_values_test, training[:,\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m], label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mActual Stress\u001b[39m\u001b[38;5;124m'\u001b[39m, color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mblue\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m plt\u001b[38;5;241m.\u001b[39mxlabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mStrain\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      6\u001b[0m plt\u001b[38;5;241m.\u001b[39mylabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mStress (MPa)\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'strain_values_test' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 800x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot predicted stress vs strain\n",
    "plt.figure(figsize=(8, 6))\n",
    "#plt.plot(strain_values_test, stress_values_predicted, label='Predicted Stress', color='red')\n",
    "plt.plot(strain_values_test, training[:,-2], label='Actual Stress', color='blue')\n",
    "plt.xlabel('Strain')\n",
    "plt.ylabel('Stress (MPa)')\n",
    "plt.title('Predicted Stress-Strain Curve')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e890c7c2-2e51-4492-b925-00fe6ceb6f3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "469da97a-7892-4e85-8b39-569d6c51a8fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "824dff1b-7af0-44f9-aa93-d7702e087c47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88fbdea-6e8d-4ce0-a153-22091daa2ab8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a54a5b4c-3bc8-4228-a216-30e355c22b81",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
